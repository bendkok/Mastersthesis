Tried running code a bunch of times. Getting meh results.

Found lr is quite important.
The code seems to get stuck in some local minima, leading to bad results after a few epochs.
It seems like the nn quite easily finds the first and last points.
It also cuts of if any periods become larger than the final point.

Maybe trying to change one of the bc will help? Or making that one not important at all.
Making the weight quite small didn't seem to help.
Changing bc didn't seem to help either.

Tried changing nn-structure.
Didn't seem to do much.

Use He instead of Golrot, better for ReLu and probably swish.


Look into output scailing. Why was that done again?
Do they actually do any input scaling? I can't find it.
They claim to do input scailing, but nothing in the code is equivalent to inp scailing. Unless it's the geometry thing.
Line 43, why?

Adding manual input scailing only made it worse.

Different activation functions didn't work.
Changing output transform to have sin didn't work.
first_num_epochs dosen't seem to do much.
lr dosen't seem to do much, other than say how many epochs we need.
Changing nn-structure didn't seem to work.
NN-weight initilization didn't work.

Good example runs:
50
62
120,90
175



Things I should write about in Method:
define FNN breifley
how you train and test
PINNs
SBINNs
what the differnet transform and weights do
FHN model
useful for neural and cardiac
stiffness of FHN - maybe later
Problems with PINNs and stiffnes
how the data was generated
hiPSC - maybe not
in silico models of hiPSC
Historic problems with fitting hiPSC models
e.g. grid search or gradient descent
Used the Yeast glycosis model/program as a starting point
some simple experiments with the nn, maybe without ode
for example feature transform or no feature transform
Autograd functionality?


Should I include units for the parameters and states?
Probably not, the equations dosen't make any sense if we do.
In the papers they seem to use states and species interchangeably. Is there a difference? So I considerthat difference?



Write down what I understand and don't, compare later.
Don't run more tests until I understand what I'm doing.

Get a number that describes how good the fit is.
E.g. norm between nn prediction and ODE prediction.
An error suited to sin-curves
Fourier
Difference between the learned and actual b-value.
Norm between blue and red curves.

I think I've missunderstood what the weights represents, will have to look further into it.

I still don't understand what theese observes are meant to be. Never mind, figured it out.
Neither what the geometry does. Is it input scailing?
They only have two data weights because there's only two obserable states (4 and 5).
Maybe I haven't missunderstod after all.

I don't understand why output_transform() does what they claim it does.
How do they calculate confidence intervals?
Fisher-Information-Matrix maybe?



" Structural non-identifiability arises from a redundant parameteriza-
tion in the formal solution of y due to insufficient mapping h of internal states x to observables
y in Eq (1) [15]." What???
I think I understand. It is non-identifiable due to the specific structure. Not quite sure why though.




